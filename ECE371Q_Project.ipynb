{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1699824617045,
     "user": {
      "displayName": "David Li",
      "userId": "11030773396771510262"
     },
     "user_tz": 360
    },
    "id": "mYlD8eYhXxNw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure\n",
    "\n",
    "\n",
    "# Loss Function: SSIM\n",
    "# Activation Function: ReLu\n",
    "\n",
    "# TODO:\n",
    "# input: 640 x 480\n",
    "# output: 1920 x 1440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1699824211646,
     "user": {
      "displayName": "David Li",
      "userId": "11030773396771510262"
     },
     "user_tz": 360
    },
    "id": "Op1dEZ7HqIwB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "INPUT_DIRECTORY = 'Data-Generator/Set1/training_data/'\n",
    "TRUTH_DIRECTORY = 'Data-Generator/Set1/ground_truth/'\n",
    "\n",
    "VAL_DIRECTORY = 'Data-Generator/Set2/training_data/'\n",
    "VAL_TRUTH_DIRECTORY = 'Data-Generator/Set2/ground_truth/'\n",
    "\n",
    "INPUT_TENSOR_DIR = 'tensors/Set1/training_data/'\n",
    "TRUTH_TENSOR_DIR = 'tensors/Set1/ground_truth/'\n",
    "\n",
    "VAL_TENSOR_DIR = 'tensors/Set2/training_data/'\n",
    "VAL_TRUTH_TENSOR_DIR = 'tensors/Set2/ground_truth/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-MeB2PWsZVO"
   },
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1699824429915,
     "user": {
      "displayName": "David Li",
      "userId": "11030773396771510262"
     },
     "user_tz": 360
    },
    "id": "m6arXK1-SmIi"
   },
   "outputs": [],
   "source": [
    "def get_data_paths(input_dir, ground_dir, num_frames):\n",
    "    data_paths = []\n",
    "    for i in range (1, num_frames - 1):\n",
    "        data_paths.append([\n",
    "            input_dir + str(i-1) + '.jpg',\n",
    "            input_dir + str(i) + '.jpg',\n",
    "            input_dir + str(i+1) + '.jpg',\n",
    "            ground_dir + str(i) + '.jpg'\n",
    "        ])\n",
    "    return data_paths\n",
    "\n",
    "def get_data_paths_offset(input_dir, ground_dir, num_frames, offset):\n",
    "    data_paths = []\n",
    "    for i in range (1, num_frames - 1):\n",
    "        data_paths.append([\n",
    "            input_dir + str(offset + i - 1) + '.jpg',\n",
    "            input_dir + str(offset + i    ) + '.jpg',\n",
    "            input_dir + str(offset + i + 1) + '.jpg',\n",
    "            ground_dir + str(offset + i   ) + '.jpg'\n",
    "        ])\n",
    "    print(len(data_paths))\n",
    "    return data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1699824431731,
     "user": {
      "displayName": "David Li",
      "userId": "11030773396771510262"
     },
     "user_tz": 360
    },
    "id": "xmeWClg3Ywux"
   },
   "outputs": [],
   "source": [
    "def get_tensor(path):\n",
    "    image_rgb = cv2.imread(path)\n",
    "    image_YCrCb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2YCrCb)\n",
    "    image_tensor = torch.tensor(image_YCrCb, dtype=torch.float32)\n",
    "    image_tensor = image_tensor / 255.0\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(data_dir, truth_dir, data_paths):\n",
    "    for idx in range(len(data_paths)):\n",
    "        tensors = []\n",
    "        for i in range(4):\n",
    "            tensors.append(get_tensor(data_paths[idx][i]))\n",
    "            \n",
    "        stacked_frames_tensor = torch.cat([tensors[0], tensors[1], tensors[2]], dim=0)\n",
    "        ground_truth_tensor = tensors[3]\n",
    "            \n",
    "        # Define the path where you want to save the tensor\n",
    "        save_path_training = data_dir + str(idx) + '.pt'\n",
    "        save_path_truth = truth_dir + str(idx) + '.pt'\n",
    "\n",
    "        torch.save(stacked_frames_tensor, save_path_training)\n",
    "        torch.save(ground_truth_tensor, save_path_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1699824433087,
     "user": {
      "displayName": "David Li",
      "userId": "11030773396771510262"
     },
     "user_tz": 360
    },
    "id": "bPPvEU4Ih-ds",
    "outputId": "ec7fb35c-0e45-4368-99f5-f0d0f8ad0e85"
   },
   "outputs": [],
   "source": [
    "class VHSDataSet(Dataset):\n",
    "    def __init__(self, data_paths, transform=None, input_tensor_dir='temp1/', truth_tensor_dir='temp2/'):\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "        self.input_tensor_dir = input_tensor_dir\n",
    "        self.truth_tensor_dir = truth_tensor_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        stacked_frames_tensor = torch.load(input_tensor_dir + str(idx) + '.pt')\n",
    "        ground_truth_tensor = torch.load(truth_tensor_dir + str(idx) + '.pt')\n",
    "        if idx % 10 == 0:\n",
    "            print(idx)\n",
    "            print(stack_frames_tensor)\n",
    "            print(ground_truth_tensor + '\\n')\n",
    "#         tensors = []\n",
    "#         for i in range(4):\n",
    "#             tensors.append(get_tensor(self.data_paths[idx][i]))\n",
    "\n",
    "#         stacked_frames_tensor = torch.cat([tensors[0], tensors[1], tensors[2]], dim=0)\n",
    "#         ground_truth_tensor = tensors[3]\n",
    "        return stacked_frames_tensor, ground_truth_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hifsfEWCpyvN"
   },
   "outputs": [],
   "source": [
    "# Create a DataLoader instance for training\n",
    "\n",
    "# data_paths = get_data_paths(INPUT_DIRECTORY, TRUTH_DIRECTORY, 2400)\n",
    "# dataset = VHSDataSet(data_paths)\n",
    "# dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO62Y95Mu0p3"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4V39Nv5OK45V"
   },
   "outputs": [],
   "source": [
    "# class ResNet(nn.Module):\n",
    "#     def __init__(self, block, layers, num_classes=1000):\n",
    "#         super(ResNet, self).__init()\n",
    "#         self.in_channels = 64\n",
    "#         self.conv1 = nn.Conv3d(3, 64, kernel_size=8, stride=2, padding=3, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "#         self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "#         self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "#         self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "#     def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "#         layers = []\n",
    "#         layers.append(block(self.in_channels, out_channels, stride))\n",
    "#         self.in_channels = out_channels * block.expansion\n",
    "#         for _ in range(1, blocks):\n",
    "#             layers.append(block(self.in_channels, out_channels))\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xtJDoF0_NBbc"
   },
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(ResidualBlock, self).__init()\n",
    "#         self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "\n",
    "#         out += residual\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VHSResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(VHSResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # 3D Convolutional layers for processing stacked frames\n",
    "        self.conv3d_1 = nn.Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Residual layers with 3D convolutions\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=(1, 2, 2))\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=(1, 2, 2))\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=(1, 2, 2))\n",
    "\n",
    "        # 2D Transposed Convolutional layers for upscaling\n",
    "        self.upscale1 = nn.ConvTranspose2d(512 * block.expansion, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upscale2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.upscale3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.ConvTranspose2d(64, 3, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is a 5D tensor with dimensions (batch_size, temporal_size, channels, height, width)\n",
    "        batch_size, temporal_size, channels, height, width = x.size()\n",
    "\n",
    "        # Reshape the input tensor for 3D convolution\n",
    "        x = x.view(batch_size * temporal_size, channels, height, width)\n",
    "\n",
    "        # 3D Convolution\n",
    "        x = self.conv3d_1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Residual layers with 3D convolutions\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # Reshape back to 4D tensor for 2D transpose convolutions\n",
    "        x = x.view(batch_size, temporal_size, -1, x.size(2), x.size(3))\n",
    "\n",
    "        # 2D Transposed Convolutions for upscaling\n",
    "        x = self.upscale1(x)\n",
    "        x = self.upscale2(x)\n",
    "        x = self.upscale3(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n",
      "tensors/Set1/training_data/0.pt\n",
      "\n",
      "tensors/Set1/training_data/1.pt\n",
      "\n",
      "tensors/Set1/training_data/2.pt\n",
      "\n",
      "tensors/Set1/training_data/3.pt\n",
      "\n",
      "tensors/Set1/training_data/4.pt\n",
      "\n",
      "tensors/Set1/training_data/5.pt\n",
      "\n",
      "tensors/Set1/training_data/6.pt\n",
      "\n",
      "tensors/Set1/training_data/7.pt\n",
      "\n",
      "tensors/Set1/training_data/8.pt\n",
      "\n",
      "tensors/Set1/training_data/9.pt\n",
      "\n",
      "tensors/Set1/training_data/10.pt\n",
      "\n",
      "tensors/Set1/training_data/11.pt\n",
      "\n",
      "tensors/Set1/training_data/12.pt\n",
      "\n",
      "tensors/Set1/training_data/13.pt\n",
      "\n",
      "tensors/Set1/training_data/14.pt\n",
      "\n",
      "tensors/Set1/training_data/15.pt\n",
      "\n",
      "tensors/Set1/training_data/16.pt\n",
      "\n",
      "tensors/Set1/training_data/17.pt\n",
      "\n",
      "tensors/Set1/training_data/18.pt\n",
      "\n",
      "tensors/Set1/training_data/19.pt\n",
      "\n",
      "tensors/Set1/training_data/20.pt\n",
      "\n",
      "tensors/Set1/training_data/21.pt\n",
      "\n",
      "tensors/Set1/training_data/22.pt\n",
      "\n",
      "tensors/Set1/training_data/23.pt\n",
      "\n",
      "tensors/Set1/training_data/24.pt\n",
      "\n",
      "tensors/Set1/training_data/25.pt\n",
      "\n",
      "tensors/Set1/training_data/26.pt\n",
      "\n",
      "tensors/Set1/training_data/27.pt\n",
      "\n",
      "tensors/Set1/training_data/28.pt\n",
      "\n",
      "tensors/Set1/training_data/29.pt\n",
      "\n",
      "tensors/Set1/training_data/30.pt\n",
      "\n",
      "tensors/Set1/training_data/31.pt\n",
      "\n",
      "tensors/Set1/training_data/32.pt\n",
      "\n",
      "tensors/Set1/training_data/33.pt\n",
      "\n",
      "tensors/Set1/training_data/34.pt\n",
      "\n",
      "tensors/Set1/training_data/35.pt\n",
      "\n",
      "tensors/Set1/training_data/36.pt\n",
      "\n",
      "tensors/Set1/training_data/37.pt\n",
      "\n",
      "tensors/Set1/training_data/38.pt\n",
      "\n",
      "tensors/Set1/training_data/39.pt\n",
      "\n",
      "tensors/Set1/training_data/40.pt\n",
      "\n",
      "tensors/Set1/training_data/41.pt\n",
      "\n",
      "tensors/Set1/training_data/42.pt\n",
      "\n",
      "tensors/Set1/training_data/43.pt\n",
      "\n",
      "tensors/Set1/training_data/44.pt\n",
      "\n",
      "tensors/Set1/training_data/45.pt\n",
      "\n",
      "tensors/Set1/training_data/46.pt\n",
      "\n",
      "tensors/Set1/training_data/47.pt\n",
      "\n",
      "tensors/Set1/training_data/48.pt\n",
      "\n",
      "tensors/Set1/training_data/49.pt\n",
      "\n",
      "tensors/Set1/training_data/50.pt\n",
      "\n",
      "tensors/Set1/training_data/51.pt\n",
      "\n",
      "tensors/Set1/training_data/52.pt\n",
      "\n",
      "tensors/Set1/training_data/53.pt\n",
      "\n",
      "tensors/Set1/training_data/54.pt\n",
      "\n",
      "tensors/Set1/training_data/55.pt\n",
      "\n",
      "tensors/Set1/training_data/56.pt\n",
      "\n",
      "tensors/Set1/training_data/57.pt\n",
      "\n",
      "tensors/Set1/training_data/58.pt\n",
      "\n",
      "tensors/Set1/training_data/59.pt\n",
      "\n",
      "tensors/Set1/training_data/60.pt\n",
      "\n",
      "tensors/Set1/training_data/61.pt\n",
      "\n",
      "tensors/Set1/training_data/62.pt\n",
      "\n",
      "tensors/Set1/training_data/63.pt\n",
      "\n",
      "tensors/Set1/training_data/64.pt\n",
      "\n",
      "tensors/Set1/training_data/65.pt\n",
      "\n",
      "tensors/Set1/training_data/66.pt\n",
      "\n",
      "tensors/Set1/training_data/67.pt\n",
      "\n",
      "tensors/Set1/training_data/68.pt\n",
      "\n",
      "tensors/Set1/training_data/69.pt\n",
      "\n",
      "tensors/Set1/training_data/70.pt\n",
      "\n",
      "tensors/Set1/training_data/71.pt\n",
      "\n",
      "tensors/Set1/training_data/72.pt\n",
      "\n",
      "tensors/Set1/training_data/73.pt\n",
      "\n",
      "tensors/Set1/training_data/74.pt\n",
      "\n",
      "tensors/Set1/training_data/75.pt\n",
      "\n",
      "tensors/Set1/training_data/76.pt\n",
      "\n",
      "tensors/Set1/training_data/77.pt\n",
      "\n",
      "tensors/Set1/training_data/78.pt\n",
      "\n",
      "tensors/Set1/training_data/79.pt\n",
      "\n",
      "tensors/Set1/training_data/80.pt\n",
      "\n",
      "tensors/Set1/training_data/81.pt\n",
      "\n",
      "tensors/Set1/training_data/82.pt\n",
      "\n",
      "tensors/Set1/training_data/83.pt\n",
      "\n",
      "tensors/Set1/training_data/84.pt\n",
      "\n",
      "tensors/Set1/training_data/85.pt\n",
      "\n",
      "tensors/Set1/training_data/86.pt\n",
      "\n",
      "tensors/Set1/training_data/87.pt\n",
      "\n",
      "tensors/Set1/training_data/88.pt\n",
      "\n",
      "tensors/Set1/training_data/89.pt\n",
      "\n",
      "tensors/Set1/training_data/90.pt\n",
      "\n",
      "tensors/Set1/training_data/91.pt\n",
      "\n",
      "tensors/Set1/training_data/92.pt\n",
      "\n",
      "tensors/Set1/training_data/93.pt\n",
      "\n",
      "tensors/Set1/training_data/94.pt\n",
      "\n",
      "tensors/Set1/training_data/95.pt\n",
      "\n",
      "tensors/Set1/training_data/96.pt\n",
      "\n",
      "tensors/Set1/training_data/97.pt\n",
      "\n",
      "tensors/Set1/training_data/98.pt\n",
      "\n",
      "tensors/Set1/training_data/99.pt\n",
      "\n",
      "tensors/Set1/training_data/100.pt\n",
      "\n",
      "tensors/Set1/training_data/101.pt\n",
      "\n",
      "tensors/Set1/training_data/102.pt\n",
      "\n",
      "tensors/Set1/training_data/103.pt\n",
      "\n",
      "tensors/Set1/training_data/104.pt\n",
      "\n",
      "tensors/Set1/training_data/105.pt\n",
      "\n",
      "tensors/Set1/training_data/106.pt\n",
      "\n",
      "tensors/Set1/training_data/107.pt\n",
      "\n",
      "tensors/Set1/training_data/108.pt\n",
      "\n",
      "tensors/Set1/training_data/109.pt\n",
      "\n",
      "tensors/Set1/training_data/110.pt\n",
      "\n",
      "tensors/Set1/training_data/111.pt\n",
      "\n",
      "tensors/Set1/training_data/112.pt\n",
      "\n",
      "tensors/Set1/training_data/113.pt\n",
      "\n",
      "tensors/Set1/training_data/114.pt\n",
      "\n",
      "tensors/Set1/training_data/115.pt\n",
      "\n",
      "tensors/Set1/training_data/116.pt\n",
      "\n",
      "tensors/Set1/training_data/117.pt\n",
      "\n",
      "tensors/Set1/training_data/118.pt\n",
      "\n",
      "tensors/Set1/training_data/119.pt\n",
      "\n",
      "tensors/Set1/training_data/120.pt\n",
      "\n",
      "tensors/Set1/training_data/121.pt\n",
      "\n",
      "tensors/Set1/training_data/122.pt\n",
      "\n",
      "tensors/Set1/training_data/123.pt\n",
      "\n",
      "tensors/Set1/training_data/124.pt\n",
      "\n",
      "tensors/Set1/training_data/125.pt\n",
      "\n",
      "tensors/Set1/training_data/126.pt\n",
      "\n",
      "tensors/Set1/training_data/127.pt\n",
      "\n",
      "tensors/Set1/training_data/128.pt\n",
      "\n",
      "tensors/Set1/training_data/129.pt\n",
      "\n",
      "tensors/Set1/training_data/130.pt\n",
      "\n",
      "tensors/Set1/training_data/131.pt\n",
      "\n",
      "tensors/Set1/training_data/132.pt\n",
      "\n",
      "tensors/Set1/training_data/133.pt\n",
      "\n",
      "tensors/Set1/training_data/134.pt\n",
      "\n",
      "tensors/Set1/training_data/135.pt\n",
      "\n",
      "tensors/Set1/training_data/136.pt\n",
      "\n",
      "tensors/Set1/training_data/137.pt\n",
      "\n",
      "tensors/Set1/training_data/138.pt\n",
      "\n",
      "tensors/Set1/training_data/139.pt\n",
      "\n",
      "tensors/Set1/training_data/140.pt\n",
      "\n",
      "tensors/Set1/training_data/141.pt\n",
      "\n",
      "tensors/Set1/training_data/142.pt\n",
      "\n",
      "tensors/Set1/training_data/143.pt\n",
      "\n",
      "tensors/Set1/training_data/144.pt\n",
      "\n",
      "tensors/Set1/training_data/145.pt\n",
      "\n",
      "tensors/Set1/training_data/146.pt\n",
      "\n",
      "tensors/Set1/training_data/147.pt\n",
      "\n",
      "tensors/Set1/training_data/148.pt\n",
      "\n",
      "tensors/Set1/training_data/149.pt\n",
      "\n",
      "tensors/Set1/training_data/150.pt\n",
      "\n",
      "tensors/Set1/training_data/151.pt\n",
      "\n",
      "tensors/Set1/training_data/152.pt\n",
      "\n",
      "tensors/Set1/training_data/153.pt\n",
      "\n",
      "tensors/Set1/training_data/154.pt\n",
      "\n",
      "tensors/Set1/training_data/155.pt\n",
      "\n",
      "tensors/Set1/training_data/156.pt\n",
      "\n",
      "tensors/Set1/training_data/157.pt\n",
      "\n",
      "tensors/Set1/training_data/158.pt\n",
      "\n",
      "tensors/Set1/training_data/159.pt\n",
      "\n",
      "tensors/Set1/training_data/160.pt\n",
      "\n",
      "tensors/Set1/training_data/161.pt\n",
      "\n",
      "tensors/Set1/training_data/162.pt\n",
      "\n",
      "tensors/Set1/training_data/163.pt\n",
      "\n",
      "tensors/Set1/training_data/164.pt\n",
      "\n",
      "tensors/Set1/training_data/165.pt\n",
      "\n",
      "tensors/Set1/training_data/166.pt\n",
      "\n",
      "tensors/Set1/training_data/167.pt\n",
      "\n",
      "tensors/Set1/training_data/168.pt\n",
      "\n",
      "tensors/Set1/training_data/169.pt\n",
      "\n",
      "tensors/Set1/training_data/170.pt\n",
      "\n",
      "tensors/Set1/training_data/171.pt\n",
      "\n",
      "tensors/Set1/training_data/172.pt\n",
      "\n",
      "tensors/Set1/training_data/173.pt\n",
      "\n",
      "tensors/Set1/training_data/174.pt\n",
      "\n",
      "tensors/Set1/training_data/175.pt\n",
      "\n",
      "tensors/Set1/training_data/176.pt\n",
      "\n",
      "tensors/Set1/training_data/177.pt\n",
      "\n",
      "tensors/Set1/training_data/178.pt\n",
      "\n",
      "tensors/Set1/training_data/179.pt\n",
      "\n",
      "tensors/Set1/training_data/180.pt\n",
      "\n",
      "tensors/Set1/training_data/181.pt\n",
      "\n",
      "tensors/Set1/training_data/182.pt\n",
      "\n",
      "tensors/Set1/training_data/183.pt\n",
      "\n",
      "tensors/Set1/training_data/184.pt\n",
      "\n",
      "tensors/Set1/training_data/185.pt\n",
      "\n",
      "tensors/Set1/training_data/186.pt\n",
      "\n",
      "tensors/Set1/training_data/187.pt\n",
      "\n",
      "tensors/Set1/training_data/188.pt\n",
      "\n",
      "tensors/Set1/training_data/189.pt\n",
      "\n",
      "tensors/Set1/training_data/190.pt\n",
      "\n",
      "tensors/Set1/training_data/191.pt\n",
      "\n",
      "tensors/Set1/training_data/192.pt\n",
      "\n",
      "tensors/Set1/training_data/193.pt\n",
      "\n",
      "tensors/Set1/training_data/194.pt\n",
      "\n",
      "tensors/Set1/training_data/195.pt\n",
      "\n",
      "tensors/Set1/training_data/196.pt\n",
      "\n",
      "tensors/Set1/training_data/197.pt\n",
      "\n",
      "tensors/Set1/training_data/198.pt\n",
      "\n",
      "tensors/Set1/training_data/199.pt\n",
      "\n",
      "tensors/Set1/training_data/200.pt\n",
      "\n",
      "tensors/Set1/training_data/201.pt\n",
      "\n",
      "tensors/Set1/training_data/202.pt\n",
      "\n",
      "tensors/Set1/training_data/203.pt\n",
      "\n",
      "tensors/Set1/training_data/204.pt\n",
      "\n",
      "tensors/Set1/training_data/205.pt\n",
      "\n",
      "tensors/Set1/training_data/206.pt\n",
      "\n",
      "tensors/Set1/training_data/207.pt\n",
      "\n",
      "tensors/Set1/training_data/208.pt\n",
      "\n",
      "tensors/Set1/training_data/209.pt\n",
      "\n",
      "tensors/Set1/training_data/210.pt\n",
      "\n",
      "tensors/Set1/training_data/211.pt\n",
      "\n",
      "tensors/Set1/training_data/212.pt\n",
      "\n",
      "tensors/Set1/training_data/213.pt\n",
      "\n",
      "tensors/Set1/training_data/214.pt\n",
      "\n",
      "tensors/Set1/training_data/215.pt\n",
      "\n",
      "tensors/Set1/training_data/216.pt\n",
      "\n",
      "tensors/Set1/training_data/217.pt\n",
      "\n",
      "tensors/Set1/training_data/218.pt\n",
      "\n",
      "tensors/Set1/training_data/219.pt\n",
      "\n",
      "tensors/Set1/training_data/220.pt\n",
      "\n",
      "tensors/Set1/training_data/221.pt\n",
      "\n",
      "tensors/Set1/training_data/222.pt\n",
      "\n",
      "tensors/Set1/training_data/223.pt\n",
      "\n",
      "tensors/Set1/training_data/224.pt\n",
      "\n",
      "tensors/Set1/training_data/225.pt\n",
      "\n",
      "tensors/Set1/training_data/226.pt\n",
      "\n",
      "tensors/Set1/training_data/227.pt\n",
      "\n",
      "tensors/Set1/training_data/228.pt\n",
      "\n",
      "tensors/Set1/training_data/229.pt\n",
      "\n",
      "tensors/Set1/training_data/230.pt\n",
      "\n",
      "tensors/Set1/training_data/231.pt\n",
      "\n",
      "tensors/Set1/training_data/232.pt\n",
      "\n",
      "tensors/Set1/training_data/233.pt\n",
      "\n",
      "tensors/Set1/training_data/234.pt\n",
      "\n",
      "tensors/Set1/training_data/235.pt\n",
      "\n",
      "tensors/Set1/training_data/236.pt\n",
      "\n",
      "tensors/Set1/training_data/237.pt\n",
      "\n",
      "tensors/Set1/training_data/238.pt\n",
      "\n",
      "tensors/Set1/training_data/239.pt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensors/Set1/training_data/240.pt\n",
      "\n",
      "tensors/Set1/training_data/241.pt\n",
      "\n",
      "tensors/Set1/training_data/242.pt\n",
      "\n",
      "tensors/Set1/training_data/243.pt\n",
      "\n",
      "tensors/Set1/training_data/244.pt\n",
      "\n",
      "tensors/Set1/training_data/245.pt\n",
      "\n",
      "tensors/Set1/training_data/246.pt\n",
      "\n",
      "tensors/Set1/training_data/247.pt\n",
      "\n",
      "tensors/Set1/training_data/248.pt\n",
      "\n",
      "tensors/Set1/training_data/249.pt\n",
      "\n",
      "tensors/Set1/training_data/250.pt\n",
      "\n",
      "tensors/Set1/training_data/251.pt\n",
      "\n",
      "tensors/Set1/training_data/252.pt\n",
      "\n",
      "tensors/Set1/training_data/253.pt\n",
      "\n",
      "tensors/Set1/training_data/254.pt\n",
      "\n",
      "tensors/Set1/training_data/255.pt\n",
      "\n",
      "tensors/Set1/training_data/256.pt\n",
      "\n",
      "tensors/Set1/training_data/257.pt\n",
      "\n",
      "tensors/Set1/training_data/258.pt\n",
      "\n",
      "tensors/Set1/training_data/259.pt\n",
      "\n",
      "tensors/Set1/training_data/260.pt\n",
      "\n",
      "tensors/Set1/training_data/261.pt\n",
      "\n",
      "tensors/Set1/training_data/262.pt\n",
      "\n",
      "tensors/Set1/training_data/263.pt\n",
      "\n",
      "tensors/Set1/training_data/264.pt\n",
      "\n",
      "tensors/Set1/training_data/265.pt\n",
      "\n",
      "tensors/Set1/training_data/266.pt\n",
      "\n",
      "tensors/Set1/training_data/267.pt\n",
      "\n",
      "tensors/Set1/training_data/268.pt\n",
      "\n",
      "tensors/Set1/training_data/269.pt\n",
      "\n",
      "tensors/Set1/training_data/270.pt\n",
      "\n",
      "tensors/Set1/training_data/271.pt\n",
      "\n",
      "tensors/Set1/training_data/272.pt\n",
      "\n",
      "tensors/Set1/training_data/273.pt\n",
      "\n",
      "tensors/Set1/training_data/274.pt\n",
      "\n",
      "tensors/Set1/training_data/275.pt\n",
      "\n",
      "tensors/Set1/training_data/276.pt\n",
      "\n",
      "tensors/Set1/training_data/277.pt\n",
      "\n",
      "tensors/Set1/training_data/278.pt\n",
      "\n",
      "tensors/Set1/training_data/279.pt\n",
      "\n",
      "tensors/Set1/training_data/280.pt\n",
      "\n",
      "tensors/Set1/training_data/281.pt\n",
      "\n",
      "tensors/Set1/training_data/282.pt\n",
      "\n",
      "tensors/Set1/training_data/283.pt\n",
      "\n",
      "tensors/Set1/training_data/284.pt\n",
      "\n",
      "tensors/Set1/training_data/285.pt\n",
      "\n",
      "tensors/Set1/training_data/286.pt\n",
      "\n",
      "tensors/Set1/training_data/287.pt\n",
      "\n",
      "tensors/Set1/training_data/288.pt\n",
      "\n",
      "tensors/Set1/training_data/289.pt\n",
      "\n",
      "tensors/Set1/training_data/290.pt\n",
      "\n",
      "tensors/Set1/training_data/291.pt\n",
      "\n",
      "tensors/Set1/training_data/292.pt\n",
      "\n",
      "tensors/Set1/training_data/293.pt\n",
      "\n",
      "tensors/Set1/training_data/294.pt\n",
      "\n",
      "tensors/Set1/training_data/295.pt\n",
      "\n",
      "tensors/Set1/training_data/296.pt\n",
      "\n",
      "tensors/Set1/training_data/297.pt\n",
      "\n",
      "tensors/Set1/training_data/298.pt\n",
      "\n",
      "tensors/Set1/training_data/299.pt\n",
      "\n",
      "tensors/Set1/training_data/300.pt\n",
      "\n",
      "tensors/Set1/training_data/301.pt\n",
      "\n",
      "tensors/Set1/training_data/302.pt\n",
      "\n",
      "tensors/Set1/training_data/303.pt\n",
      "\n",
      "tensors/Set1/training_data/304.pt\n",
      "\n",
      "tensors/Set1/training_data/305.pt\n",
      "\n",
      "tensors/Set1/training_data/306.pt\n",
      "\n",
      "tensors/Set1/training_data/307.pt\n",
      "\n",
      "tensors/Set1/training_data/308.pt\n",
      "\n",
      "tensors/Set1/training_data/309.pt\n",
      "\n",
      "tensors/Set1/training_data/310.pt\n",
      "\n",
      "tensors/Set1/training_data/311.pt\n",
      "\n",
      "tensors/Set1/training_data/312.pt\n",
      "\n",
      "tensors/Set1/training_data/313.pt\n",
      "\n",
      "tensors/Set1/training_data/314.pt\n",
      "\n",
      "tensors/Set1/training_data/315.pt\n",
      "\n",
      "tensors/Set1/training_data/316.pt\n",
      "\n",
      "tensors/Set1/training_data/317.pt\n",
      "\n",
      "tensors/Set1/training_data/318.pt\n",
      "\n",
      "tensors/Set1/training_data/319.pt\n",
      "\n",
      "tensors/Set1/training_data/320.pt\n",
      "\n",
      "tensors/Set1/training_data/321.pt\n",
      "\n",
      "tensors/Set1/training_data/322.pt\n",
      "\n",
      "tensors/Set1/training_data/323.pt\n",
      "\n",
      "tensors/Set1/training_data/324.pt\n",
      "\n",
      "tensors/Set1/training_data/325.pt\n",
      "\n",
      "tensors/Set1/training_data/326.pt\n",
      "\n",
      "tensors/Set1/training_data/327.pt\n",
      "\n",
      "tensors/Set1/training_data/328.pt\n",
      "\n",
      "tensors/Set1/training_data/329.pt\n",
      "\n",
      "tensors/Set1/training_data/330.pt\n",
      "\n",
      "tensors/Set1/training_data/331.pt\n",
      "\n",
      "tensors/Set1/training_data/332.pt\n",
      "\n",
      "tensors/Set1/training_data/333.pt\n",
      "\n",
      "tensors/Set1/training_data/334.pt\n",
      "\n",
      "tensors/Set1/training_data/335.pt\n",
      "\n",
      "tensors/Set1/training_data/336.pt\n",
      "\n",
      "tensors/Set1/training_data/337.pt\n",
      "\n",
      "tensors/Set1/training_data/338.pt\n",
      "\n",
      "tensors/Set1/training_data/339.pt\n",
      "\n",
      "tensors/Set1/training_data/340.pt\n",
      "\n",
      "tensors/Set1/training_data/341.pt\n",
      "\n",
      "tensors/Set1/training_data/342.pt\n",
      "\n",
      "tensors/Set1/training_data/343.pt\n",
      "\n",
      "tensors/Set1/training_data/344.pt\n",
      "\n",
      "tensors/Set1/training_data/345.pt\n",
      "\n",
      "tensors/Set1/training_data/346.pt\n",
      "\n",
      "tensors/Set1/training_data/347.pt\n",
      "\n",
      "tensors/Set1/training_data/348.pt\n",
      "\n",
      "tensors/Set1/training_data/349.pt\n",
      "\n",
      "tensors/Set1/training_data/350.pt\n",
      "\n",
      "tensors/Set1/training_data/351.pt\n",
      "\n",
      "tensors/Set1/training_data/352.pt\n",
      "\n",
      "tensors/Set1/training_data/353.pt\n",
      "\n",
      "tensors/Set1/training_data/354.pt\n",
      "\n",
      "tensors/Set1/training_data/355.pt\n",
      "\n",
      "tensors/Set1/training_data/356.pt\n",
      "\n",
      "tensors/Set1/training_data/357.pt\n",
      "\n",
      "tensors/Set1/training_data/358.pt\n",
      "\n",
      "tensors/Set1/training_data/359.pt\n",
      "\n",
      "tensors/Set1/training_data/360.pt\n",
      "\n",
      "tensors/Set1/training_data/361.pt\n",
      "\n",
      "tensors/Set1/training_data/362.pt\n",
      "\n",
      "tensors/Set1/training_data/363.pt\n",
      "\n",
      "tensors/Set1/training_data/364.pt\n",
      "\n",
      "tensors/Set1/training_data/365.pt\n",
      "\n",
      "tensors/Set1/training_data/366.pt\n",
      "\n",
      "tensors/Set1/training_data/367.pt\n",
      "\n",
      "tensors/Set1/training_data/368.pt\n",
      "\n",
      "tensors/Set1/training_data/369.pt\n",
      "\n",
      "tensors/Set1/training_data/370.pt\n",
      "\n",
      "tensors/Set1/training_data/371.pt\n",
      "\n",
      "tensors/Set1/training_data/372.pt\n",
      "\n",
      "tensors/Set1/training_data/373.pt\n",
      "\n",
      "tensors/Set1/training_data/374.pt\n",
      "\n",
      "tensors/Set1/training_data/375.pt\n",
      "\n",
      "tensors/Set1/training_data/376.pt\n",
      "\n",
      "tensors/Set1/training_data/377.pt\n",
      "\n",
      "tensors/Set1/training_data/378.pt\n",
      "\n",
      "tensors/Set1/training_data/379.pt\n",
      "\n",
      "tensors/Set1/training_data/380.pt\n",
      "\n",
      "tensors/Set1/training_data/381.pt\n",
      "\n",
      "tensors/Set1/training_data/382.pt\n",
      "\n",
      "tensors/Set1/training_data/383.pt\n",
      "\n",
      "tensors/Set1/training_data/384.pt\n",
      "\n",
      "tensors/Set1/training_data/385.pt\n",
      "\n",
      "tensors/Set1/training_data/386.pt\n",
      "\n",
      "tensors/Set1/training_data/387.pt\n",
      "\n",
      "tensors/Set1/training_data/388.pt\n",
      "\n",
      "tensors/Set1/training_data/389.pt\n",
      "\n",
      "tensors/Set1/training_data/390.pt\n",
      "\n",
      "tensors/Set1/training_data/391.pt\n",
      "\n",
      "tensors/Set1/training_data/392.pt\n",
      "\n",
      "tensors/Set1/training_data/393.pt\n",
      "\n",
      "tensors/Set1/training_data/394.pt\n",
      "\n",
      "tensors/Set1/training_data/395.pt\n",
      "\n",
      "tensors/Set1/training_data/396.pt\n",
      "\n",
      "tensors/Set1/training_data/397.pt\n",
      "\n",
      "tensors/Set1/training_data/398.pt\n",
      "\n",
      "tensors/Set1/training_data/399.pt\n",
      "\n",
      "tensors/Set1/training_data/400.pt\n",
      "\n",
      "tensors/Set1/training_data/401.pt\n",
      "\n",
      "tensors/Set1/training_data/402.pt\n",
      "\n",
      "tensors/Set1/training_data/403.pt\n",
      "\n",
      "tensors/Set1/training_data/404.pt\n",
      "\n",
      "tensors/Set1/training_data/405.pt\n",
      "\n",
      "tensors/Set1/training_data/406.pt\n",
      "\n",
      "tensors/Set1/training_data/407.pt\n",
      "\n",
      "tensors/Set1/training_data/408.pt\n",
      "\n",
      "tensors/Set1/training_data/409.pt\n",
      "\n",
      "tensors/Set1/training_data/410.pt\n",
      "\n",
      "tensors/Set1/training_data/411.pt\n",
      "\n",
      "tensors/Set1/training_data/412.pt\n",
      "\n",
      "tensors/Set1/training_data/413.pt\n",
      "\n",
      "tensors/Set1/training_data/414.pt\n",
      "\n",
      "tensors/Set1/training_data/415.pt\n",
      "\n",
      "tensors/Set1/training_data/416.pt\n",
      "\n",
      "tensors/Set1/training_data/417.pt\n",
      "\n",
      "tensors/Set1/training_data/418.pt\n",
      "\n",
      "tensors/Set1/training_data/419.pt\n",
      "\n",
      "tensors/Set1/training_data/420.pt\n",
      "\n",
      "tensors/Set1/training_data/421.pt\n",
      "\n",
      "tensors/Set1/training_data/422.pt\n",
      "\n",
      "tensors/Set1/training_data/423.pt\n",
      "\n",
      "tensors/Set1/training_data/424.pt\n",
      "\n",
      "tensors/Set1/training_data/425.pt\n",
      "\n",
      "tensors/Set1/training_data/426.pt\n",
      "\n",
      "tensors/Set1/training_data/427.pt\n",
      "\n",
      "tensors/Set1/training_data/428.pt\n",
      "\n",
      "tensors/Set1/training_data/429.pt\n",
      "\n",
      "tensors/Set1/training_data/430.pt\n",
      "\n",
      "tensors/Set1/training_data/431.pt\n",
      "\n",
      "tensors/Set1/training_data/432.pt\n",
      "\n",
      "tensors/Set1/training_data/433.pt\n",
      "\n",
      "tensors/Set1/training_data/434.pt\n",
      "\n",
      "tensors/Set1/training_data/435.pt\n",
      "\n",
      "tensors/Set1/training_data/436.pt\n",
      "\n",
      "tensors/Set1/training_data/437.pt\n",
      "\n",
      "tensors/Set1/training_data/438.pt\n",
      "\n",
      "tensors/Set1/training_data/439.pt\n",
      "\n",
      "tensors/Set1/training_data/440.pt\n",
      "\n",
      "tensors/Set1/training_data/441.pt\n",
      "\n",
      "tensors/Set1/training_data/442.pt\n",
      "\n",
      "tensors/Set1/training_data/443.pt\n",
      "\n",
      "tensors/Set1/training_data/444.pt\n",
      "\n",
      "tensors/Set1/training_data/445.pt\n",
      "\n",
      "tensors/Set1/training_data/446.pt\n",
      "\n",
      "tensors/Set1/training_data/447.pt\n",
      "\n",
      "tensors/Set1/training_data/448.pt\n",
      "\n",
      "tensors/Set1/training_data/449.pt\n",
      "\n",
      "tensors/Set1/training_data/450.pt\n",
      "\n",
      "tensors/Set1/training_data/451.pt\n",
      "\n",
      "tensors/Set1/training_data/452.pt\n",
      "\n",
      "tensors/Set1/training_data/453.pt\n",
      "\n",
      "tensors/Set1/training_data/454.pt\n",
      "\n",
      "tensors/Set1/training_data/455.pt\n",
      "\n",
      "tensors/Set1/training_data/456.pt\n",
      "\n",
      "tensors/Set1/training_data/457.pt\n",
      "\n",
      "tensors/Set1/training_data/458.pt\n",
      "\n",
      "tensors/Set1/training_data/459.pt\n",
      "\n",
      "tensors/Set1/training_data/460.pt\n",
      "\n",
      "tensors/Set1/training_data/461.pt\n",
      "\n",
      "tensors/Set1/training_data/462.pt\n",
      "\n",
      "tensors/Set1/training_data/463.pt\n",
      "\n",
      "tensors/Set1/training_data/464.pt\n",
      "\n",
      "tensors/Set1/training_data/465.pt\n",
      "\n",
      "tensors/Set1/training_data/466.pt\n",
      "\n",
      "tensors/Set1/training_data/467.pt\n",
      "\n",
      "tensors/Set1/training_data/468.pt\n",
      "\n",
      "tensors/Set1/training_data/469.pt\n",
      "\n",
      "tensors/Set1/training_data/470.pt\n",
      "\n",
      "tensors/Set1/training_data/471.pt\n",
      "\n",
      "tensors/Set1/training_data/472.pt\n",
      "\n",
      "tensors/Set1/training_data/473.pt\n",
      "\n",
      "tensors/Set1/training_data/474.pt\n",
      "\n",
      "tensors/Set1/training_data/475.pt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensors/Set1/training_data/476.pt\n",
      "\n",
      "tensors/Set1/training_data/477.pt\n",
      "\n",
      "tensors/Set1/training_data/478.pt\n",
      "\n",
      "tensors/Set1/training_data/479.pt\n",
      "\n",
      "tensors/Set1/training_data/480.pt\n",
      "\n",
      "tensors/Set1/training_data/481.pt\n",
      "\n",
      "tensors/Set1/training_data/482.pt\n",
      "\n",
      "tensors/Set1/training_data/483.pt\n",
      "\n",
      "tensors/Set1/training_data/484.pt\n",
      "\n",
      "tensors/Set1/training_data/485.pt\n",
      "\n",
      "tensors/Set1/training_data/486.pt\n",
      "\n",
      "tensors/Set1/training_data/487.pt\n",
      "\n",
      "tensors/Set1/training_data/488.pt\n",
      "\n",
      "tensors/Set1/training_data/489.pt\n",
      "\n",
      "tensors/Set1/training_data/490.pt\n",
      "\n",
      "tensors/Set1/training_data/491.pt\n",
      "\n",
      "tensors/Set1/training_data/492.pt\n",
      "\n",
      "tensors/Set1/training_data/493.pt\n",
      "\n",
      "tensors/Set1/training_data/494.pt\n",
      "\n",
      "tensors/Set1/training_data/495.pt\n",
      "\n",
      "tensors/Set1/training_data/496.pt\n",
      "\n",
      "tensors/Set1/training_data/497.pt\n",
      "\n",
      "tensors/Set1/training_data/498.pt\n",
      "\n",
      "tensors/Set1/training_data/499.pt\n",
      "\n",
      "tensors/Set1/training_data/500.pt\n",
      "\n",
      "tensors/Set1/training_data/501.pt\n",
      "\n",
      "tensors/Set1/training_data/502.pt\n",
      "\n",
      "tensors/Set1/training_data/503.pt\n",
      "\n",
      "tensors/Set1/training_data/504.pt\n",
      "\n",
      "tensors/Set1/training_data/505.pt\n",
      "\n",
      "tensors/Set1/training_data/506.pt\n",
      "\n",
      "tensors/Set1/training_data/507.pt\n",
      "\n",
      "tensors/Set1/training_data/508.pt\n",
      "\n",
      "tensors/Set1/training_data/509.pt\n",
      "\n",
      "tensors/Set1/training_data/510.pt\n",
      "\n",
      "tensors/Set1/training_data/511.pt\n",
      "\n",
      "tensors/Set1/training_data/512.pt\n",
      "\n",
      "tensors/Set1/training_data/513.pt\n",
      "\n",
      "tensors/Set1/training_data/514.pt\n",
      "\n",
      "tensors/Set1/training_data/515.pt\n",
      "\n",
      "tensors/Set1/training_data/516.pt\n",
      "\n",
      "tensors/Set1/training_data/517.pt\n",
      "\n",
      "tensors/Set1/training_data/518.pt\n",
      "\n",
      "tensors/Set1/training_data/519.pt\n",
      "\n",
      "tensors/Set1/training_data/520.pt\n",
      "\n",
      "tensors/Set1/training_data/521.pt\n",
      "\n",
      "tensors/Set1/training_data/522.pt\n",
      "\n",
      "tensors/Set1/training_data/523.pt\n",
      "\n",
      "tensors/Set1/training_data/524.pt\n",
      "\n",
      "tensors/Set1/training_data/525.pt\n",
      "\n",
      "tensors/Set1/training_data/526.pt\n",
      "\n",
      "tensors/Set1/training_data/527.pt\n",
      "\n",
      "tensors/Set1/training_data/528.pt\n",
      "\n",
      "tensors/Set1/training_data/529.pt\n",
      "\n",
      "tensors/Set1/training_data/530.pt\n",
      "\n",
      "tensors/Set1/training_data/531.pt\n",
      "\n",
      "tensors/Set1/training_data/532.pt\n",
      "\n",
      "tensors/Set1/training_data/533.pt\n",
      "\n",
      "tensors/Set1/training_data/534.pt\n",
      "\n",
      "tensors/Set1/training_data/535.pt\n",
      "\n",
      "tensors/Set1/training_data/536.pt\n",
      "\n",
      "tensors/Set1/training_data/537.pt\n",
      "\n",
      "tensors/Set1/training_data/538.pt\n",
      "\n",
      "tensors/Set1/training_data/539.pt\n",
      "\n",
      "tensors/Set1/training_data/540.pt\n",
      "\n",
      "tensors/Set1/training_data/541.pt\n",
      "\n",
      "tensors/Set1/training_data/542.pt\n",
      "\n",
      "tensors/Set1/training_data/543.pt\n",
      "\n",
      "tensors/Set1/training_data/544.pt\n",
      "\n",
      "tensors/Set1/training_data/545.pt\n",
      "\n",
      "tensors/Set1/training_data/546.pt\n",
      "\n",
      "tensors/Set1/training_data/547.pt\n",
      "\n",
      "tensors/Set1/training_data/548.pt\n",
      "\n",
      "tensors/Set1/training_data/549.pt\n",
      "\n",
      "tensors/Set1/training_data/550.pt\n",
      "\n",
      "tensors/Set1/training_data/551.pt\n",
      "\n",
      "tensors/Set1/training_data/552.pt\n",
      "\n",
      "tensors/Set1/training_data/553.pt\n",
      "\n",
      "tensors/Set1/training_data/554.pt\n",
      "\n",
      "tensors/Set1/training_data/555.pt\n",
      "\n",
      "tensors/Set1/training_data/556.pt\n",
      "\n",
      "tensors/Set1/training_data/557.pt\n",
      "\n",
      "tensors/Set1/training_data/558.pt\n",
      "\n",
      "tensors/Set1/training_data/559.pt\n",
      "\n",
      "tensors/Set1/training_data/560.pt\n",
      "\n",
      "tensors/Set1/training_data/561.pt\n",
      "\n",
      "tensors/Set1/training_data/562.pt\n",
      "\n",
      "tensors/Set1/training_data/563.pt\n",
      "\n",
      "tensors/Set1/training_data/564.pt\n",
      "\n",
      "tensors/Set1/training_data/565.pt\n",
      "\n",
      "tensors/Set1/training_data/566.pt\n",
      "\n",
      "tensors/Set1/training_data/567.pt\n",
      "\n",
      "tensors/Set1/training_data/568.pt\n",
      "\n",
      "tensors/Set1/training_data/569.pt\n",
      "\n",
      "tensors/Set1/training_data/570.pt\n",
      "\n",
      "tensors/Set1/training_data/571.pt\n",
      "\n",
      "tensors/Set1/training_data/572.pt\n",
      "\n",
      "tensors/Set1/training_data/573.pt\n",
      "\n",
      "tensors/Set1/training_data/574.pt\n",
      "\n",
      "tensors/Set1/training_data/575.pt\n",
      "\n",
      "tensors/Set1/training_data/576.pt\n",
      "\n",
      "tensors/Set1/training_data/577.pt\n",
      "\n",
      "tensors/Set1/training_data/578.pt\n",
      "\n",
      "tensors/Set1/training_data/579.pt\n",
      "\n",
      "tensors/Set1/training_data/580.pt\n",
      "\n",
      "tensors/Set1/training_data/581.pt\n",
      "\n",
      "tensors/Set1/training_data/582.pt\n",
      "\n",
      "tensors/Set1/training_data/583.pt\n",
      "\n",
      "tensors/Set1/training_data/584.pt\n",
      "\n",
      "tensors/Set1/training_data/585.pt\n",
      "\n",
      "tensors/Set1/training_data/586.pt\n",
      "\n",
      "tensors/Set1/training_data/587.pt\n",
      "\n",
      "tensors/Set1/training_data/588.pt\n",
      "\n",
      "tensors/Set1/training_data/589.pt\n",
      "\n",
      "tensors/Set1/training_data/590.pt\n",
      "\n",
      "tensors/Set1/training_data/591.pt\n",
      "\n",
      "tensors/Set1/training_data/592.pt\n",
      "\n",
      "tensors/Set1/training_data/593.pt\n",
      "\n",
      "tensors/Set1/training_data/594.pt\n",
      "\n",
      "tensors/Set1/training_data/595.pt\n",
      "\n",
      "tensors/Set1/training_data/596.pt\n",
      "\n",
      "tensors/Set1/training_data/597.pt\n",
      "\n",
      "tensors/Set1/training_data/598.pt\n",
      "\n",
      "tensors/Set1/training_data/599.pt\n",
      "\n",
      "tensors/Set1/training_data/600.pt\n",
      "\n",
      "tensors/Set1/training_data/601.pt\n",
      "\n",
      "tensors/Set1/training_data/602.pt\n",
      "\n",
      "tensors/Set1/training_data/603.pt\n",
      "\n",
      "tensors/Set1/training_data/604.pt\n",
      "\n",
      "tensors/Set1/training_data/605.pt\n",
      "\n",
      "tensors/Set1/training_data/606.pt\n",
      "\n",
      "tensors/Set1/training_data/607.pt\n",
      "\n",
      "tensors/Set1/training_data/608.pt\n",
      "\n",
      "tensors/Set1/training_data/609.pt\n",
      "\n",
      "tensors/Set1/training_data/610.pt\n",
      "\n",
      "tensors/Set1/training_data/611.pt\n",
      "\n",
      "tensors/Set1/training_data/612.pt\n",
      "\n",
      "tensors/Set1/training_data/613.pt\n",
      "\n",
      "tensors/Set1/training_data/614.pt\n",
      "\n",
      "tensors/Set1/training_data/615.pt\n",
      "\n",
      "tensors/Set1/training_data/616.pt\n",
      "\n",
      "tensors/Set1/training_data/617.pt\n",
      "\n",
      "tensors/Set1/training_data/618.pt\n",
      "\n",
      "tensors/Set1/training_data/619.pt\n",
      "\n",
      "tensors/Set1/training_data/620.pt\n",
      "\n",
      "tensors/Set1/training_data/621.pt\n",
      "\n",
      "tensors/Set1/training_data/622.pt\n",
      "\n",
      "tensors/Set1/training_data/623.pt\n",
      "\n",
      "tensors/Set1/training_data/624.pt\n",
      "\n",
      "tensors/Set1/training_data/625.pt\n",
      "\n",
      "tensors/Set1/training_data/626.pt\n",
      "\n",
      "tensors/Set1/training_data/627.pt\n",
      "\n",
      "tensors/Set1/training_data/628.pt\n",
      "\n",
      "tensors/Set1/training_data/629.pt\n",
      "\n",
      "tensors/Set1/training_data/630.pt\n",
      "\n",
      "tensors/Set1/training_data/631.pt\n",
      "\n",
      "tensors/Set1/training_data/632.pt\n",
      "\n",
      "tensors/Set1/training_data/633.pt\n",
      "\n",
      "tensors/Set1/training_data/634.pt\n",
      "\n",
      "tensors/Set1/training_data/635.pt\n",
      "\n",
      "tensors/Set1/training_data/636.pt\n",
      "\n",
      "tensors/Set1/training_data/637.pt\n",
      "\n",
      "tensors/Set1/training_data/638.pt\n",
      "\n",
      "tensors/Set1/training_data/639.pt\n",
      "\n",
      "tensors/Set1/training_data/640.pt\n",
      "\n",
      "tensors/Set1/training_data/641.pt\n",
      "\n",
      "tensors/Set1/training_data/642.pt\n",
      "\n",
      "tensors/Set1/training_data/643.pt\n",
      "\n",
      "tensors/Set1/training_data/644.pt\n",
      "\n",
      "tensors/Set1/training_data/645.pt\n",
      "\n",
      "tensors/Set1/training_data/646.pt\n",
      "\n",
      "tensors/Set1/training_data/647.pt\n",
      "\n",
      "tensors/Set1/training_data/648.pt\n",
      "\n",
      "tensors/Set1/training_data/649.pt\n",
      "\n",
      "tensors/Set1/training_data/650.pt\n",
      "\n",
      "tensors/Set1/training_data/651.pt\n",
      "\n",
      "tensors/Set1/training_data/652.pt\n",
      "\n",
      "tensors/Set1/training_data/653.pt\n",
      "\n",
      "tensors/Set1/training_data/654.pt\n",
      "\n",
      "tensors/Set1/training_data/655.pt\n",
      "\n",
      "tensors/Set1/training_data/656.pt\n",
      "\n",
      "tensors/Set1/training_data/657.pt\n",
      "\n",
      "tensors/Set1/training_data/658.pt\n",
      "\n",
      "tensors/Set1/training_data/659.pt\n",
      "\n",
      "tensors/Set1/training_data/660.pt\n",
      "\n",
      "tensors/Set1/training_data/661.pt\n",
      "\n",
      "tensors/Set1/training_data/662.pt\n",
      "\n",
      "tensors/Set1/training_data/663.pt\n",
      "\n",
      "tensors/Set1/training_data/664.pt\n",
      "\n",
      "tensors/Set1/training_data/665.pt\n",
      "\n",
      "tensors/Set1/training_data/666.pt\n",
      "\n",
      "tensors/Set1/training_data/667.pt\n",
      "\n",
      "tensors/Set1/training_data/668.pt\n",
      "\n",
      "tensors/Set1/training_data/669.pt\n",
      "\n",
      "tensors/Set1/training_data/670.pt\n",
      "\n",
      "tensors/Set1/training_data/671.pt\n",
      "\n",
      "tensors/Set1/training_data/672.pt\n",
      "\n",
      "tensors/Set1/training_data/673.pt\n",
      "\n",
      "tensors/Set1/training_data/674.pt\n",
      "\n",
      "tensors/Set1/training_data/675.pt\n",
      "\n",
      "tensors/Set1/training_data/676.pt\n",
      "\n",
      "tensors/Set1/training_data/677.pt\n",
      "\n",
      "tensors/Set1/training_data/678.pt\n",
      "\n",
      "tensors/Set1/training_data/679.pt\n",
      "\n",
      "tensors/Set1/training_data/680.pt\n",
      "\n",
      "tensors/Set1/training_data/681.pt\n",
      "\n",
      "tensors/Set1/training_data/682.pt\n",
      "\n",
      "tensors/Set1/training_data/683.pt\n",
      "\n",
      "tensors/Set1/training_data/684.pt\n",
      "\n",
      "tensors/Set1/training_data/685.pt\n",
      "\n",
      "tensors/Set1/training_data/686.pt\n",
      "\n",
      "tensors/Set1/training_data/687.pt\n",
      "\n",
      "tensors/Set1/training_data/688.pt\n",
      "\n",
      "tensors/Set1/training_data/689.pt\n",
      "\n",
      "tensors/Set1/training_data/690.pt\n",
      "\n",
      "tensors/Set1/training_data/691.pt\n",
      "\n",
      "tensors/Set1/training_data/692.pt\n",
      "\n",
      "tensors/Set1/training_data/693.pt\n",
      "\n",
      "tensors/Set1/training_data/694.pt\n",
      "\n",
      "tensors/Set1/training_data/695.pt\n",
      "\n",
      "tensors/Set1/training_data/696.pt\n",
      "\n",
      "tensors/Set1/training_data/697.pt\n",
      "\n",
      "tensors/Set1/training_data/698.pt\n",
      "\n",
      "tensors/Set1/training_data/699.pt\n",
      "\n",
      "tensors/Set1/training_data/700.pt\n",
      "\n",
      "tensors/Set1/training_data/701.pt\n",
      "\n",
      "tensors/Set1/training_data/702.pt\n",
      "\n",
      "tensors/Set1/training_data/703.pt\n",
      "\n",
      "tensors/Set1/training_data/704.pt\n",
      "\n",
      "tensors/Set1/training_data/705.pt\n",
      "\n",
      "tensors/Set1/training_data/706.pt\n",
      "\n",
      "tensors/Set1/training_data/707.pt\n",
      "\n",
      "tensors/Set1/training_data/708.pt\n",
      "\n",
      "tensors/Set1/training_data/709.pt\n",
      "\n",
      "tensors/Set1/training_data/710.pt\n",
      "\n",
      "tensors/Set1/training_data/711.pt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensors/Set1/training_data/712.pt\n",
      "\n",
      "tensors/Set1/training_data/713.pt\n",
      "\n",
      "tensors/Set1/training_data/714.pt\n",
      "\n",
      "tensors/Set1/training_data/715.pt\n",
      "\n",
      "tensors/Set1/training_data/716.pt\n",
      "\n",
      "tensors/Set1/training_data/717.pt\n",
      "\n",
      "tensors/Set1/training_data/718.pt\n",
      "\n",
      "tensors/Set1/training_data/719.pt\n",
      "\n",
      "tensors/Set1/training_data/720.pt\n",
      "\n",
      "tensors/Set1/training_data/721.pt\n",
      "\n",
      "tensors/Set1/training_data/722.pt\n",
      "\n",
      "tensors/Set1/training_data/723.pt\n",
      "\n",
      "tensors/Set1/training_data/724.pt\n",
      "\n",
      "tensors/Set1/training_data/725.pt\n",
      "\n",
      "tensors/Set1/training_data/726.pt\n",
      "\n",
      "tensors/Set1/training_data/727.pt\n",
      "\n",
      "tensors/Set1/training_data/728.pt\n",
      "\n",
      "tensors/Set1/training_data/729.pt\n",
      "\n",
      "tensors/Set1/training_data/730.pt\n",
      "\n",
      "tensors/Set1/training_data/731.pt\n",
      "\n",
      "tensors/Set1/training_data/732.pt\n",
      "\n",
      "tensors/Set1/training_data/733.pt\n",
      "\n",
      "tensors/Set1/training_data/734.pt\n",
      "\n",
      "tensors/Set1/training_data/735.pt\n",
      "\n",
      "tensors/Set1/training_data/736.pt\n",
      "\n",
      "tensors/Set1/training_data/737.pt\n",
      "\n",
      "tensors/Set1/training_data/738.pt\n",
      "\n",
      "tensors/Set1/training_data/739.pt\n",
      "\n",
      "tensors/Set1/training_data/740.pt\n",
      "\n",
      "tensors/Set1/training_data/741.pt\n",
      "\n",
      "tensors/Set1/training_data/742.pt\n",
      "\n",
      "tensors/Set1/training_data/743.pt\n",
      "\n",
      "tensors/Set1/training_data/744.pt\n",
      "\n",
      "tensors/Set1/training_data/745.pt\n",
      "\n",
      "tensors/Set1/training_data/746.pt\n",
      "\n",
      "tensors/Set1/training_data/747.pt\n",
      "\n",
      "tensors/Set1/training_data/748.pt\n",
      "\n",
      "tensors/Set1/training_data/749.pt\n",
      "\n",
      "tensors/Set1/training_data/750.pt\n",
      "\n",
      "tensors/Set1/training_data/751.pt\n",
      "\n",
      "tensors/Set1/training_data/752.pt\n",
      "\n",
      "tensors/Set1/training_data/753.pt\n",
      "\n",
      "tensors/Set1/training_data/754.pt\n",
      "\n",
      "tensors/Set1/training_data/755.pt\n",
      "\n",
      "tensors/Set1/training_data/756.pt\n",
      "\n",
      "tensors/Set1/training_data/757.pt\n",
      "\n",
      "tensors/Set1/training_data/758.pt\n",
      "\n",
      "tensors/Set1/training_data/759.pt\n",
      "\n",
      "tensors/Set1/training_data/760.pt\n",
      "\n",
      "tensors/Set1/training_data/761.pt\n",
      "\n",
      "tensors/Set1/training_data/762.pt\n",
      "\n",
      "tensors/Set1/training_data/763.pt\n",
      "\n",
      "tensors/Set1/training_data/764.pt\n",
      "\n",
      "tensors/Set1/training_data/765.pt\n",
      "\n",
      "tensors/Set1/training_data/766.pt\n",
      "\n",
      "tensors/Set1/training_data/767.pt\n",
      "\n",
      "tensors/Set1/training_data/768.pt\n",
      "\n",
      "tensors/Set1/training_data/769.pt\n",
      "\n",
      "tensors/Set1/training_data/770.pt\n",
      "\n",
      "tensors/Set1/training_data/771.pt\n",
      "\n",
      "tensors/Set1/training_data/772.pt\n",
      "\n",
      "tensors/Set1/training_data/773.pt\n",
      "\n",
      "tensors/Set1/training_data/774.pt\n",
      "\n",
      "tensors/Set1/training_data/775.pt\n",
      "\n",
      "tensors/Set1/training_data/776.pt\n",
      "\n",
      "tensors/Set1/training_data/777.pt\n",
      "\n",
      "tensors/Set1/training_data/778.pt\n",
      "\n",
      "tensors/Set1/training_data/779.pt\n",
      "\n",
      "tensors/Set1/training_data/780.pt\n",
      "\n",
      "tensors/Set1/training_data/781.pt\n",
      "\n",
      "tensors/Set1/training_data/782.pt\n",
      "\n",
      "tensors/Set1/training_data/783.pt\n",
      "\n",
      "tensors/Set1/training_data/784.pt\n",
      "\n",
      "tensors/Set1/training_data/785.pt\n",
      "\n",
      "tensors/Set1/training_data/786.pt\n",
      "\n",
      "tensors/Set1/training_data/787.pt\n",
      "\n",
      "tensors/Set1/training_data/788.pt\n",
      "\n",
      "tensors/Set1/training_data/789.pt\n",
      "\n",
      "tensors/Set1/training_data/790.pt\n",
      "\n",
      "tensors/Set1/training_data/791.pt\n",
      "\n",
      "tensors/Set1/training_data/792.pt\n",
      "\n",
      "tensors/Set1/training_data/793.pt\n",
      "\n",
      "tensors/Set1/training_data/794.pt\n",
      "\n",
      "tensors/Set1/training_data/795.pt\n",
      "\n",
      "tensors/Set1/training_data/796.pt\n",
      "\n",
      "tensors/Set1/training_data/797.pt\n",
      "\n",
      "tensors/Set1/training_data/798.pt\n",
      "\n",
      "tensors/Set1/training_data/799.pt\n",
      "\n",
      "tensors/Set1/training_data/800.pt\n",
      "\n",
      "tensors/Set1/training_data/801.pt\n",
      "\n",
      "tensors/Set1/training_data/802.pt\n",
      "\n",
      "tensors/Set1/training_data/803.pt\n",
      "\n",
      "tensors/Set1/training_data/804.pt\n",
      "\n",
      "tensors/Set1/training_data/805.pt\n",
      "\n",
      "tensors/Set1/training_data/806.pt\n",
      "\n",
      "tensors/Set1/training_data/807.pt\n",
      "\n",
      "tensors/Set1/training_data/808.pt\n",
      "\n",
      "tensors/Set1/training_data/809.pt\n",
      "\n",
      "tensors/Set1/training_data/810.pt\n",
      "\n",
      "tensors/Set1/training_data/811.pt\n",
      "\n",
      "tensors/Set1/training_data/812.pt\n",
      "\n",
      "tensors/Set1/training_data/813.pt\n",
      "\n",
      "tensors/Set1/training_data/814.pt\n",
      "\n",
      "tensors/Set1/training_data/815.pt\n",
      "\n",
      "tensors/Set1/training_data/816.pt\n",
      "\n",
      "tensors/Set1/training_data/817.pt\n",
      "\n",
      "tensors/Set1/training_data/818.pt\n",
      "\n",
      "tensors/Set1/training_data/819.pt\n",
      "\n",
      "tensors/Set1/training_data/820.pt\n",
      "\n",
      "tensors/Set1/training_data/821.pt\n",
      "\n",
      "tensors/Set1/training_data/822.pt\n",
      "\n",
      "tensors/Set1/training_data/823.pt\n",
      "\n",
      "tensors/Set1/training_data/824.pt\n",
      "\n",
      "tensors/Set1/training_data/825.pt\n",
      "\n",
      "tensors/Set1/training_data/826.pt\n",
      "\n",
      "tensors/Set1/training_data/827.pt\n",
      "\n",
      "tensors/Set1/training_data/828.pt\n",
      "\n",
      "tensors/Set1/training_data/829.pt\n",
      "\n",
      "tensors/Set1/training_data/830.pt\n",
      "\n",
      "tensors/Set1/training_data/831.pt\n",
      "\n",
      "tensors/Set1/training_data/832.pt\n",
      "\n",
      "tensors/Set1/training_data/833.pt\n",
      "\n",
      "tensors/Set1/training_data/834.pt\n",
      "\n",
      "tensors/Set1/training_data/835.pt\n",
      "\n",
      "tensors/Set1/training_data/836.pt\n",
      "\n",
      "tensors/Set1/training_data/837.pt\n",
      "\n",
      "tensors/Set1/training_data/838.pt\n",
      "\n",
      "tensors/Set1/training_data/839.pt\n",
      "\n",
      "tensors/Set1/training_data/840.pt\n",
      "\n",
      "tensors/Set1/training_data/841.pt\n",
      "\n",
      "tensors/Set1/training_data/842.pt\n",
      "\n",
      "tensors/Set1/training_data/843.pt\n",
      "\n",
      "tensors/Set1/training_data/844.pt\n",
      "\n",
      "tensors/Set1/training_data/845.pt\n",
      "\n",
      "tensors/Set1/training_data/846.pt\n",
      "\n",
      "tensors/Set1/training_data/847.pt\n",
      "\n",
      "tensors/Set1/training_data/848.pt\n",
      "\n",
      "tensors/Set1/training_data/849.pt\n",
      "\n",
      "tensors/Set1/training_data/850.pt\n",
      "\n",
      "tensors/Set1/training_data/851.pt\n",
      "\n",
      "tensors/Set1/training_data/852.pt\n",
      "\n",
      "tensors/Set1/training_data/853.pt\n",
      "\n",
      "tensors/Set1/training_data/854.pt\n",
      "\n",
      "tensors/Set1/training_data/855.pt\n",
      "\n",
      "tensors/Set1/training_data/856.pt\n",
      "\n",
      "tensors/Set1/training_data/857.pt\n",
      "\n",
      "tensors/Set1/training_data/858.pt\n",
      "\n",
      "tensors/Set1/training_data/859.pt\n",
      "\n",
      "tensors/Set1/training_data/860.pt\n",
      "\n",
      "tensors/Set1/training_data/861.pt\n",
      "\n",
      "tensors/Set1/training_data/862.pt\n",
      "\n",
      "tensors/Set1/training_data/863.pt\n",
      "\n",
      "tensors/Set1/training_data/864.pt\n",
      "\n",
      "tensors/Set1/training_data/865.pt\n",
      "\n",
      "tensors/Set1/training_data/866.pt\n",
      "\n",
      "tensors/Set1/training_data/867.pt\n",
      "\n",
      "tensors/Set1/training_data/868.pt\n",
      "\n",
      "tensors/Set1/training_data/869.pt\n",
      "\n",
      "tensors/Set1/training_data/870.pt\n",
      "\n",
      "tensors/Set1/training_data/871.pt\n",
      "\n",
      "tensors/Set1/training_data/872.pt\n",
      "\n",
      "tensors/Set1/training_data/873.pt\n",
      "\n",
      "tensors/Set1/training_data/874.pt\n",
      "\n",
      "tensors/Set1/training_data/875.pt\n",
      "\n",
      "tensors/Set1/training_data/876.pt\n",
      "\n",
      "tensors/Set1/training_data/877.pt\n",
      "\n",
      "tensors/Set1/training_data/878.pt\n",
      "\n",
      "tensors/Set1/training_data/879.pt\n",
      "\n",
      "tensors/Set1/training_data/880.pt\n",
      "\n",
      "tensors/Set1/training_data/881.pt\n",
      "\n",
      "tensors/Set1/training_data/882.pt\n",
      "\n",
      "tensors/Set1/training_data/883.pt\n",
      "\n",
      "tensors/Set1/training_data/884.pt\n",
      "\n",
      "tensors/Set1/training_data/885.pt\n",
      "\n",
      "tensors/Set1/training_data/886.pt\n",
      "\n",
      "tensors/Set1/training_data/887.pt\n",
      "\n",
      "tensors/Set1/training_data/888.pt\n",
      "\n",
      "tensors/Set1/training_data/889.pt\n",
      "\n",
      "tensors/Set1/training_data/890.pt\n",
      "\n",
      "tensors/Set1/training_data/891.pt\n",
      "\n",
      "tensors/Set1/training_data/892.pt\n",
      "\n",
      "tensors/Set1/training_data/893.pt\n",
      "\n",
      "tensors/Set1/training_data/894.pt\n",
      "\n",
      "tensors/Set1/training_data/895.pt\n",
      "\n",
      "tensors/Set1/training_data/896.pt\n",
      "\n",
      "tensors/Set1/training_data/897.pt\n",
      "\n",
      "tensors/Set1/training_data/898.pt\n",
      "\n",
      "tensors/Set1/training_data/899.pt\n",
      "\n",
      "tensors/Set1/training_data/900.pt\n",
      "\n",
      "tensors/Set1/training_data/901.pt\n",
      "\n",
      "tensors/Set1/training_data/902.pt\n",
      "\n",
      "tensors/Set1/training_data/903.pt\n",
      "\n",
      "tensors/Set1/training_data/904.pt\n",
      "\n",
      "tensors/Set1/training_data/905.pt\n",
      "\n",
      "tensors/Set1/training_data/906.pt\n",
      "\n",
      "tensors/Set1/training_data/907.pt\n",
      "\n",
      "tensors/Set1/training_data/908.pt\n",
      "\n",
      "tensors/Set1/training_data/909.pt\n",
      "\n",
      "tensors/Set1/training_data/910.pt\n",
      "\n",
      "tensors/Set1/training_data/911.pt\n",
      "\n",
      "tensors/Set1/training_data/912.pt\n",
      "\n",
      "tensors/Set1/training_data/913.pt\n",
      "\n",
      "tensors/Set1/training_data/914.pt\n",
      "\n",
      "tensors/Set1/training_data/915.pt\n",
      "\n",
      "tensors/Set1/training_data/916.pt\n",
      "\n",
      "tensors/Set1/training_data/917.pt\n",
      "\n",
      "tensors/Set1/training_data/918.pt\n",
      "\n",
      "tensors/Set1/training_data/919.pt\n",
      "\n",
      "tensors/Set1/training_data/920.pt\n",
      "\n",
      "tensors/Set1/training_data/921.pt\n",
      "\n",
      "tensors/Set1/training_data/922.pt\n",
      "\n",
      "tensors/Set1/training_data/923.pt\n",
      "\n",
      "tensors/Set1/training_data/924.pt\n",
      "\n",
      "tensors/Set1/training_data/925.pt\n",
      "\n",
      "tensors/Set1/training_data/926.pt\n",
      "\n",
      "tensors/Set1/training_data/927.pt\n",
      "\n",
      "tensors/Set1/training_data/928.pt\n",
      "\n",
      "tensors/Set1/training_data/929.pt\n",
      "\n",
      "tensors/Set1/training_data/930.pt\n",
      "\n",
      "tensors/Set1/training_data/931.pt\n",
      "\n",
      "tensors/Set1/training_data/932.pt\n",
      "\n",
      "tensors/Set1/training_data/933.pt\n",
      "\n",
      "tensors/Set1/training_data/934.pt\n",
      "\n",
      "tensors/Set1/training_data/935.pt\n",
      "\n",
      "tensors/Set1/training_data/936.pt\n",
      "\n",
      "tensors/Set1/training_data/937.pt\n",
      "\n",
      "tensors/Set1/training_data/938.pt\n",
      "\n",
      "tensors/Set1/training_data/939.pt\n",
      "\n",
      "tensors/Set1/training_data/940.pt\n",
      "\n",
      "tensors/Set1/training_data/941.pt\n",
      "\n",
      "tensors/Set1/training_data/942.pt\n",
      "\n",
      "tensors/Set1/training_data/943.pt\n",
      "\n",
      "tensors/Set1/training_data/944.pt\n",
      "\n",
      "tensors/Set1/training_data/945.pt\n",
      "\n",
      "tensors/Set1/training_data/946.pt\n",
      "\n",
      "tensors/Set1/training_data/947.pt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensors/Set1/training_data/948.pt\n",
      "\n",
      "tensors/Set1/training_data/949.pt\n",
      "\n",
      "tensors/Set1/training_data/950.pt\n",
      "\n",
      "tensors/Set1/training_data/951.pt\n",
      "\n",
      "tensors/Set1/training_data/952.pt\n",
      "\n",
      "tensors/Set1/training_data/953.pt\n",
      "\n",
      "tensors/Set1/training_data/954.pt\n",
      "\n",
      "tensors/Set1/training_data/955.pt\n",
      "\n",
      "tensors/Set1/training_data/956.pt\n",
      "\n",
      "tensors/Set1/training_data/957.pt\n",
      "\n",
      "tensors/Set1/training_data/958.pt\n",
      "\n",
      "tensors/Set1/training_data/959.pt\n",
      "\n",
      "tensors/Set1/training_data/960.pt\n",
      "\n",
      "tensors/Set1/training_data/961.pt\n",
      "\n",
      "tensors/Set1/training_data/962.pt\n",
      "\n",
      "tensors/Set1/training_data/963.pt\n",
      "\n",
      "tensors/Set1/training_data/964.pt\n",
      "\n",
      "tensors/Set1/training_data/965.pt\n",
      "\n",
      "tensors/Set1/training_data/966.pt\n",
      "\n",
      "tensors/Set1/training_data/967.pt\n",
      "\n",
      "tensors/Set1/training_data/968.pt\n",
      "\n",
      "tensors/Set1/training_data/969.pt\n",
      "\n",
      "tensors/Set1/training_data/970.pt\n",
      "\n",
      "tensors/Set1/training_data/971.pt\n",
      "\n",
      "tensors/Set1/training_data/972.pt\n",
      "\n",
      "tensors/Set1/training_data/973.pt\n",
      "\n",
      "tensors/Set1/training_data/974.pt\n",
      "\n",
      "tensors/Set1/training_data/975.pt\n",
      "\n",
      "tensors/Set1/training_data/976.pt\n",
      "\n",
      "tensors/Set1/training_data/977.pt\n",
      "\n",
      "tensors/Set1/training_data/978.pt\n",
      "\n",
      "tensors/Set1/training_data/979.pt\n",
      "\n",
      "tensors/Set1/training_data/980.pt\n",
      "\n",
      "tensors/Set1/training_data/981.pt\n",
      "\n",
      "tensors/Set1/training_data/982.pt\n",
      "\n",
      "tensors/Set1/training_data/983.pt\n",
      "\n",
      "tensors/Set1/training_data/984.pt\n",
      "\n",
      "tensors/Set1/training_data/985.pt\n",
      "\n",
      "tensors/Set1/training_data/986.pt\n",
      "\n",
      "tensors/Set1/training_data/987.pt\n",
      "\n",
      "tensors/Set1/training_data/988.pt\n",
      "\n",
      "tensors/Set1/training_data/989.pt\n",
      "\n",
      "tensors/Set1/training_data/990.pt\n",
      "\n",
      "tensors/Set1/training_data/991.pt\n",
      "\n",
      "tensors/Set1/training_data/992.pt\n",
      "\n",
      "tensors/Set1/training_data/993.pt\n",
      "\n",
      "tensors/Set1/training_data/994.pt\n",
      "\n",
      "tensors/Set1/training_data/995.pt\n",
      "\n",
      "tensors/Set1/training_data/996.pt\n",
      "\n",
      "tensors/Set1/training_data/997.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_paths = get_data_paths_offset(INPUT_DIRECTORY, TRUTH_DIRECTORY, 1000, 6000)\n",
    "generate_dataset(INPUT_TENSOR_DIR, TRUTH_TENSOR_DIR, train_data_paths)\n",
    "train_dataset = VHSDataSet(data_paths=train_data_paths, input_tensor_dir = INPUT_TENSOR_DIR, truth_tensor_dir = TRUTH_TENSOR_DIR)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "val_data_paths = get_data_paths_offset(VAL_DIRECTORY, VAL_TRUTH_DIRECTORY, 200, 3800)\n",
    "generate_dataset(VAL_TENSOR_DIR, VAL_TRUTH_TENSOR_DIR, val_data_paths)\n",
    "val_dataset = VHSDataSet(data_paths=val_data_paths, input_tensor_dir = VAL_TENSOR_DIR, truth_tensor_dir = VAL_TRUTH_TENSOR_DIR)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.VHSDataSet'>: it's not the same object as __main__.VHSDataSet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     13\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cv\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[38;5;241m.\u001b[39mdump(obj)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <class '__main__.VHSDataSet'>: it's not the same object as __main__.VHSDataSet"
     ]
    }
   ],
   "source": [
    "model = VHSResNet(block=ResidualBlock, layers=[2, 2, 2, 2])\n",
    "ssim_loss = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Convert outputs and targets to images for SSIM calculation\n",
    "        outputs_img = to_pil_image(outputs.clamp(0, 1))\n",
    "        targets_img = to_pil_image(targets.clamp(0, 1))\n",
    "\n",
    "        # Calculate SSIM loss\n",
    "        loss = 1 - ssim_loss(outputs_img, targets_img)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Convert outputs and targets to images for SSIM calculation\n",
    "            outputs_img = to_pil_image(outputs.clamp(0, 1))\n",
    "            targets_img = to_pil_image(targets.clamp(0, 1))\n",
    "\n",
    "            # Calculate SSIM loss\n",
    "            val_loss = 1 - ssim_loss(outputs_img, targets_img)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training SSIM Loss: {loss.item()}, Validation SSIM Loss: {val_loss.item()}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"vhs_restoration_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START: 7:04 PM\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKQxXZ3M9lIhRrkIj+pRMn",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
